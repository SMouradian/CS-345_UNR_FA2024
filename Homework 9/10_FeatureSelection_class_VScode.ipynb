{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdx22uAF7gT5"
      },
      "source": [
        "# Feature Selection\n",
        "\n",
        "In this class, we're going to address the problem of WAY TOO MANY features.\n",
        "\n",
        "Think Bag of Words for Fake News detection. Our vocabulary was WAY too big, which caused all kinds of problems!\n",
        "\n",
        "So we're going to look at a few feature selection techniques and then you can try them out on the fake news data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!brew install python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install scikit-learn\n",
        "!pip3 install matplotlib\n",
        "!pip3 install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ7dgu_i7cW7",
        "outputId": "8fee67aa-1568-4606-f880-f06852829f11"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
        "print(X)\n",
        "sel = VarianceThreshold(threshold=0.8*(1-0.8))\n",
        "X_new = sel.fit_transform(X)\n",
        "print('-'*60)\n",
        "print(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3E3Ga_A8Of6"
      },
      "source": [
        "## Removing Features with Low Variance\n",
        "\n",
        "*  Variance is a statistical measure that describes the spread or dispersion of a set of data points. Specifically, it quantifies how far each number in the set is from the mean (average) and, thus, from every other number in the set. In other words, variance provides a measure of the variability or volatility of a dataset.\n",
        "* Features with low variance might not contain much information. If a feature has almost the same value for all the samples in a dataset, it might not be very discriminative.\n",
        "*  VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
        "\n",
        "As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by p*(1-p).\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T6RUPbO8kkX"
      },
      "source": [
        "As expected, VarianceThreshold has removed the first column, which has a probability p=5/6 > 0.8 of containing a zero.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCEFjSUH9Lse"
      },
      "source": [
        "## Univariate Feature Selection\n",
        "\n",
        "* Univariate feature selection is a statistical method used in machine learning and data analysis to select the most informative features from a dataset. It evaluates each feature individually to determine the strength of the relationship of the feature with the response variable. The main idea is to keep the best features based on certain criteria and discard the less informative ones.\n",
        "\n",
        "Scikit-learn exposes feature selection routines as objects that implement the transform method:\n",
        "\n",
        "- SelectKBest removes all but the k highest scoring features\n",
        "- SelectPercentile removes all but a user-specified highest scoring percentage of features using common univariate statistical tests for each feature: false positive rate SelectFpr, false discovery rate SelectFdr, or family wise error SelectFwe.\n",
        "- Generic Univariate Select allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsvm-i0Z_TRB"
      },
      "source": [
        "###Iris Dataset\n",
        "https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyswx3wI_ExL",
        "outputId": "a73ccbba-6818-4ca9-d843-3ee9829f81c9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "print(X.shape)\n",
        "\n",
        "\n",
        "X_new = SelectKBest(f_classif, k=3).fit_transform(X, y)\n",
        "\n",
        "print(X_new.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw2mNDXP_-Na"
      },
      "source": [
        "## L1-based Feature Selection\n",
        "\n",
        "L1-based feature selection refers to the use of L1 regularization (also known as Lasso regularization) to induce sparsity in the coefficients of a linear model, which in turn can be used to select important features. When L1 regularization is applied, many feature coefficients become exactly zero, effectively excluding those features from the model. The features with non-zero coefficients are considered to be the selected or important features.\n",
        "\n",
        "---\n",
        "* L1 Regularization: In linear regression, the objective is to minimize the sum of squared residuals. With L1 regularization, an additional term is added to this objective: the sum of the absolute values of the coefficients (i.e., model weights), multiplied by a regularization parameter (often denoted as α or λ).\n",
        "\n",
        "* The objective becomes: Minimize(sum of squared residuals + λ × sum of absolute values of coefficients)\n",
        "\n",
        "* As λ increases, the penalty for non-zero coefficients also increases, pushing more coefficients to become exactly zero.\n",
        "---\n",
        "\n",
        "When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with SelectFromModel to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the Lasso for regression, and of LogisticRegression and LinearSVC for classification:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z6tQuO4Suqi"
      },
      "source": [
        "A **Support Vector Machine (SVM)** is a supervised machine learning algorithm used for classification tasks (and sometimes for regression). It aims to find the best boundary (or hyperplane) that separates data points from different classes. Here’s a breakdown of how an SVM classifier works:\n",
        "\n",
        "**Key Concepts**\n",
        "\n",
        "**Hyperplane:**\n",
        "\n",
        "* In SVM, the goal is to find a hyperplane that best divides a dataset into classes.\n",
        "\n",
        "* In a 2D space, this hyperplane is a line, in 3D space, it's a plane, and in higher dimensions, it becomes a hyperplane.\n",
        "\n",
        "For example, in a 2D feature space with two features \\\\(x_1\\\\) and \\\\(x_2\\\\), the decision boundary is defined by:\n",
        "\n",
        "\\\\(w_1 ⋅ x_1 + w_2 ⋅ x_2 + b = 0\\\\)\n",
        "\n",
        "where:\n",
        "\n",
        "- \\\\(w_1\\\\) and \\\\(w_2\\\\) are the weights (or coefficients) for the features \\\\(x_1\\\\) and \\\\(x_2\\\\).\n",
        "- \\\\(b\\\\) is the intercept (bias).\n",
        "- The values of \\\\(x_1\\\\) and \\\\(x_2\\\\) determine the input data points.\n",
        "\n",
        "\n",
        "* The best hyperplane is the one that maximizes the margin between the two classes.\n",
        "\n",
        "\n",
        "**Margin:**\n",
        "\n",
        "* The margin is the distance between the hyperplane and the nearest data points from each class. These nearest points are called support vectors.\n",
        "\n",
        "* SVM maximizes this margin to improve the generalization ability of the model. The larger the margin, the better the separation between classes.\n",
        "\n",
        "**Support Vectors:**\n",
        "\n",
        "Support vectors are the data points that are closest to the hyperplane. These points are critical because the position of the hyperplane depends on them.\n",
        "Even if other data points are changed, as long as the support vectors stay the same, the hyperplane remains unchanged.\n",
        "\n",
        "**Linear and Non-Linear Classification:**\n",
        "\n",
        "Linear SVM: If the data is linearly separable, meaning you can separate the classes with a straight line or hyperplane, SVM works by finding that optimal hyperplane.\n",
        "\n",
        "Non-Linear SVM: When data is not linearly separable, SVM uses the kernel trick to map the data into a higher-dimensional space where it becomes linearly separable. Popular kernels include:\n",
        "* Polynomial kernel\n",
        "* Radial Basis Function (RBF) kernel\n",
        "\n",
        "**SVM Variants**\n",
        "\n",
        "**LinearSVC:**\n",
        "\n",
        "This is a variant of SVM used when the data is linearly separable. It uses a linear kernel and is faster for large datasets compared to non-linear SVM.\n",
        "\n",
        "**Soft Margin SVM:**\n",
        "\n",
        "In real-world data, it's common to have some overlap between classes. Soft Margin SVM allows some misclassification of points but tries to minimize it. The C parameter controls this trade-off between maximizing the margin and minimizing the classification error.\n",
        "* A large C value aims for fewer misclassifications but at the cost of a smaller margin.\n",
        "* A small C value allows a larger margin but permits more misclassifications.\n",
        "\n",
        "**Kernel SVM:**\n",
        "\n",
        "If the data cannot be separated by a straight line, kernel methods (like RBF or polynomial) allow the SVM to classify non-linear data by transforming the input space into higher dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVV5dy2VAIxP",
        "outputId": "4323e9ba-5c27-4ed1-e482-af11a33597d8"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "print(X.shape)\n",
        "\n",
        "lsvc = LinearSVC(penalty='l1', C=0.01, dual=False, max_iter=10000)\n",
        "lsvc = lsvc.fit(X,y)\n",
        "\n",
        "print(lsvc.coef_) # # Weights for class 1, 2, 3\n",
        "\n",
        "model = SelectFromModel(lsvc, prefit=True) # uses the mean of the coefficients across all classes.\n",
        "X_new = model.transform(X)\n",
        "\n",
        "print(X_new.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh4fUt3tAWjR"
      },
      "source": [
        "##Tree-based Feature Selection\n",
        "* Tree-based feature selection leverages the structure and properties of tree-based machine learning models, such as decision trees, random forests, and gradient-boosted trees, to rank or select important features. These models inherently perform feature selection by choosing the best features to split on at each node of the tree.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn-ensemble-extratreesclassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcfD710_Ag-M",
        "outputId": "b93e3d6d-2041-431b-d7be-c677bd965d12"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "print(X.shape)\n",
        "\n",
        "clf = ExtraTreesClassifier(n_estimators=100).fit(X,y)\n",
        "print(clf.feature_importances_)\n",
        "\n",
        "X_new = SelectFromModel(clf, prefit=True).transform(X)\n",
        "\n",
        "print(X_new.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHTfGkOBA00y"
      },
      "source": [
        "# Homework: Try Feature Selection on Fake News Dataset (Due next Monday)\n",
        "\n",
        "\n",
        "Use feature selection techniques to reduce the size of the vocabulary! Try any of the above techniques or other ones you feel like: https://scikit-learn.org/stable/modules/feature_selection.html\n",
        "\n",
        "After feature selection, use our 4 classification models (decision tree, KNN, neural network) to classify the Fake/Real News data (use 80% for training and 20% for testing, like usual)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CjS844DXbZA"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  # lower case\n",
        "  text = text.lower()\n",
        "  # initializing punctuations string\n",
        "  punctuations = '''1234567890!@#$%^&*()-=_+[]{}\\|;\"':,./<>?`~“”’‘''' # use ''' ''' three quotes\n",
        "  for element in punctuations:\n",
        "    text = text.replace(element, '')\n",
        "  # split into words\n",
        "  text=text.strip().split()\n",
        "\n",
        "  # remove links:\n",
        "  text = [x for x in text if 'www' not in x] # short-way of looping operations for list data\n",
        "  text = [x for x in text if 'http' not in x]\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcOUOLyMX6v7",
        "outputId": "9d23e8fb-a7b2-4ecd-ef9e-e42c49369193"
      },
      "outputs": [],
      "source": [
        "# generate a training dataset using 80% of the data\n",
        "\n",
        "import pandas\n",
        "\n",
        "folder_path = '/workspaces/CS-345_UNR_FA2024/Homework 9/' # my google drive folder path\n",
        "news_folder_path = folder_path + 'news_dataset/'\n",
        "\n",
        "df_fake = pandas.read_csv(news_folder_path + \"Fake.csv\")['text']\n",
        "df_real = pandas.read_csv(news_folder_path + \"True.csv\")['text']\n",
        "print('number of fake news samples: {}'.format(len(df_fake)))\n",
        "print('number of true news samples: {}'.format(len(df_real)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQtg2uRYe73g",
        "outputId": "7ac61a28-da09-45cf-e90e-a8d888d259b9"
      },
      "outputs": [],
      "source": [
        "#Get Vocab Words for Fake\n",
        "word_dict = {}\n",
        "for text in df_fake:\n",
        "  text = clean_text(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "#Get Vocab Words for Real\n",
        "for text in df_real:\n",
        "  text = clean_text(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "\n",
        "#Remove words that occur less than min_thresh times and more than max_thresh times:\n",
        "# we still need to shorten the feature dimension manually, as google colab cannot deal with this large feature space.\n",
        "vocab = list(word_dict)\n",
        "print(\"Vocabulary Length Before Min/Max Removal:\", len(vocab))\n",
        "\n",
        "min_thresh = 100\n",
        "max_thresh = 5000\n",
        "for word in vocab:\n",
        "  if word_dict[word] <= min_thresh or word_dict[word] > max_thresh:\n",
        "    word_dict.pop(word)\n",
        "\n",
        "vocab = list(word_dict)\n",
        "print(\"Vocabulary Length After Min/Max Removal:\", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fRnT0Exjqqy"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Function to process the texts and generate feature vectors\n",
        "def process_texts(texts, label):\n",
        "    X_data = []  # To store feature vectors\n",
        "    y_data = []  # To store labels\n",
        "\n",
        "    # Process each text\n",
        "    for text in texts:\n",
        "        # Clean the text\n",
        "        text = clean_text(text)\n",
        "\n",
        "        # Use defaultdict to avoid checking if word exists\n",
        "        article_dict = defaultdict(int)\n",
        "\n",
        "        # Count word occurrences\n",
        "        for word in text:\n",
        "            article_dict[word] += 1\n",
        "\n",
        "        # Turn the count dictionary into a list of values corresponding to the vocabulary\n",
        "        article_list = [article_dict[word] for word in vocab]\n",
        "\n",
        "        # Append the feature vector and the label\n",
        "        X_data.append(article_list)\n",
        "        y_data.append(label)\n",
        "\n",
        "    return X_data, y_data\n",
        "\n",
        "# Initialize data containers\n",
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "# Process both fake and real news texts and collect the data\n",
        "X_fake, y_fake = process_texts(df_fake, 1)  # Label 1 for fake news\n",
        "X_real, y_real = process_texts(df_real, 0)  # Label 0 for real news\n",
        "\n",
        "# Combine the data\n",
        "X_all.extend(X_fake)\n",
        "X_all.extend(X_real)\n",
        "y_all.extend(y_fake)\n",
        "y_all.extend(y_real)\n",
        "\n",
        "# At this point, X_all contains the feature vectors, and y_all contains the corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etSbFg64j1PC",
        "outputId": "a6ec9a0d-17b8-4946-9488-baffcdc1d381"
      },
      "outputs": [],
      "source": [
        "# Convert X_all and y_all to appropriate formats if necessary (e.g., NumPy arrays)\n",
        "import numpy as np\n",
        "X_all = np.array(X_all)\n",
        "y_all = np.array(y_all)\n",
        "\n",
        "# Verify the shape of the dataset\n",
        "print(f\"Shape of feature dataset: {X_all.shape}\")\n",
        "print(f\"Shape of labels: {y_all.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## without feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q3uYblilT44",
        "outputId": "c0ec667a-f24b-40ac-fc5a-a3b386ae817b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# standarization the whole dataset\n",
        "X_all_scaled = scaler.fit_transform(X_all)\n",
        "\n",
        "# # Split the data into training (80%) and testing (20%)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all_scaled, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify the shape of the data\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# model training\n",
        "nn = MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), batch_size=128, max_iter=1000, random_state=1)\n",
        "\n",
        "nn = nn.fit(X_train, y_train)\n",
        "print('training loss', nn.best_loss_)\n",
        "plt.plot(nn.loss_curve_, '-.', color='red', label='nn 2')\n",
        "plt.ylabel('training loss')\n",
        "plt.xlabel('# of iterations')\n",
        "\n",
        "# testing\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "y_pred = nn.predict(X_test_scaled)\n",
        "print('testing accuracy', accuracy_score(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing during the training\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "nn = MLPClassifier(solver='adam', hidden_layer_sizes=(5, 2), batch_size=128,\n",
        "                   max_iter=1, warm_start=True, random_state=1)\n",
        "\n",
        "# Lists to store training and testing accuracy\n",
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "# Lists to store training and testing loss\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Train the model for multiple iterations\n",
        "for i in range(100):  # 100 iterations\n",
        "\n",
        "    # Incremental Training: partial_fit allows you to\n",
        "    # incrementally fit the model and track the performance after each iteration.\n",
        "\n",
        "    if i == 0:\n",
        "        nn.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
        "    else:\n",
        "        nn.partial_fit(X_train, y_train)\n",
        "\n",
        "    # Append the training loss (stored in nn.loss_curve_)\n",
        "    train_losses.append(nn.loss_)\n",
        "\n",
        "    # Evaluate on training set\n",
        "    train_pred = nn.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_pred)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_pred = nn.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    y_test_prob = nn.predict_proba(X_test)  # Get predicted probabilities\n",
        "    test_loss = log_loss(y_test, y_test_prob)  # Compute the log loss for the test set\n",
        "    test_losses.append(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the test accuracy over iterations\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test and Train Accuracy over Iterations')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and testing loss over iterations\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(test_losses, label=\"Testing Loss\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Testing Loss over Iterations')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## with feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-uL7gBHamUQ",
        "outputId": "535c0e22-0584-4b76-bf1c-1ffae38eb6de"
      },
      "outputs": [],
      "source": [
        "## feature selection\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_all_scaled = scaler.fit_transform(X_all)\n",
        "\n",
        "lsvc = LinearSVC(penalty='l1', C=0.01, dual=False, max_iter=10000)\n",
        "lsvc = lsvc.fit(X_all_scaled,y_all)\n",
        "\n",
        "model = SelectFromModel(lsvc, prefit=True) # uses the mean of the coefficients across all classes.\n",
        "X_new = model.transform(X_all_scaled)\n",
        "\n",
        "print('shape of new data features: {}'.format(X_new.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V89G7zotiIbs",
        "outputId": "b452d43a-f8d9-4c6b-f2eb-2fe8338af874"
      },
      "outputs": [],
      "source": [
        "# Split the data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify the shape of the data\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# model training\n",
        "nn = MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), batch_size=128, max_iter=1000, random_state=1)\n",
        "\n",
        "nn = nn.fit(X_train, y_train)\n",
        "print(nn.best_loss_)\n",
        "plt.plot(nn.loss_curve_, '-.', color='red', label='nn 2')\n",
        "plt.ylabel('training loss')\n",
        "plt.xlabel('# of iterations')\n",
        "\n",
        "# testing after training\n",
        "y_pred = nn.predict(X_test)\n",
        "print(accuracy_score(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing during the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing during the training\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "nn = MLPClassifier(solver='adam', hidden_layer_sizes=(5, 2), batch_size=128,\n",
        "                   max_iter=1, warm_start=True, random_state=1)\n",
        "\n",
        "# Lists to store training and testing accuracy\n",
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "# Lists to store training and testing loss\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Train the model for multiple iterations\n",
        "for i in range(100):  # 100 iterations\n",
        "\n",
        "    # Incremental Training: partial_fit allows you to\n",
        "    # incrementally fit the model and track the performance after each iteration.\n",
        "\n",
        "    if i == 0:\n",
        "        nn.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
        "    else:\n",
        "        nn.partial_fit(X_train, y_train)\n",
        "\n",
        "    # Append the training loss (stored in nn.loss_curve_)\n",
        "    train_losses.append(nn.loss_)\n",
        "\n",
        "    # Evaluate on training set\n",
        "    train_pred = nn.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_pred)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_pred = nn.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    y_test_prob = nn.predict_proba(X_test)  # Get predicted probabilities\n",
        "    test_loss = log_loss(y_test, y_test_prob)  # Compute the log loss for the test set\n",
        "    test_losses.append(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the test accuracy over iterations\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test and Train Accuracy over Iterations')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and testing loss over iterations\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(test_losses, label=\"Testing Loss\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Testing Loss over Iterations')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## feature selection without standarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## feature selection without standarization\n",
        "lsvc = LinearSVC(penalty='l1', C=0.01, dual=False, max_iter=10000)\n",
        "lsvc = lsvc.fit(X_all,y_all)\n",
        "\n",
        "model = SelectFromModel(lsvc, prefit=True) # uses the mean of the coefficients across all classes.\n",
        "X_new = model.transform(X_all_scaled)\n",
        "\n",
        "print('shape of new data features: {}'.format(X_new.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify the shape of the data\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# model training\n",
        "nn = MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), batch_size=128, max_iter=1000, random_state=1)\n",
        "\n",
        "nn = nn.fit(X_train, y_train)\n",
        "print(nn.best_loss_)\n",
        "plt.plot(nn.loss_curve_, '-.', color='red', label='nn 2')\n",
        "plt.ylabel('training loss')\n",
        "plt.xlabel('# of iterations')\n",
        "\n",
        "# testing after training\n",
        "y_pred = nn.predict(X_test)\n",
        "print(accuracy_score(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing during the training\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "nn = MLPClassifier(solver='adam', hidden_layer_sizes=(5, 2), batch_size=128,\n",
        "                   max_iter=1, warm_start=True, random_state=1)\n",
        "\n",
        "# Lists to store training and testing accuracy\n",
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "# Lists to store training and testing loss\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Train the model for multiple iterations\n",
        "for i in range(100):  # 100 iterations\n",
        "\n",
        "    # Incremental Training: partial_fit allows you to\n",
        "    # incrementally fit the model and track the performance after each iteration.\n",
        "\n",
        "    if i == 0:\n",
        "        nn.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
        "    else:\n",
        "        nn.partial_fit(X_train, y_train)\n",
        "\n",
        "    # Append the training loss (stored in nn.loss_curve_)\n",
        "    train_losses.append(nn.loss_)\n",
        "\n",
        "    # Evaluate on training set\n",
        "    train_pred = nn.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_pred)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_pred = nn.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    y_test_prob = nn.predict_proba(X_test)  # Get predicted probabilities\n",
        "    test_loss = log_loss(y_test, y_test_prob)  # Compute the log loss for the test set\n",
        "    test_losses.append(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the test accuracy over iterations\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test and Train Accuracy over Iterations')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and testing loss over iterations\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(test_losses, label=\"Testing Loss\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Testing Loss over Iterations')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## how to improve it? A better feature selection method?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DjkIX7Gk6vj"
      },
      "outputs": [],
      "source": [
        "# Homeworks: Try other feature selection methods and re-train the NN model \n",
        "# # to see which feature selection works best on this dataset\n",
        "\n",
        "#USED UNIVARIATE FEATURE SELECTION\n",
        "## feature selection\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "print(X.shape)\n",
        "\n",
        "\n",
        "X_new = SelectKBest(f_classif, k=3).fit_transform(X, y)\n",
        "\n",
        "print(X_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  # lower case\n",
        "  text = text.lower()\n",
        "  # initializing punctuations string\n",
        "  punctuations = '''1234567890!@#$%^&*()-=_+[]{}\\|;\"':,./<>?`~“”’‘''' # use ''' ''' three quotes\n",
        "  for element in punctuations:\n",
        "    text = text.replace(element, '')\n",
        "  # split into words\n",
        "  text=text.strip().split()\n",
        "\n",
        "  # remove links:\n",
        "  text = [x for x in text if 'www' not in x] # short-way of looping operations for list data\n",
        "  text = [x for x in text if 'http' not in x]\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate a training dataset using 80% of the data\n",
        "\n",
        "import pandas\n",
        "\n",
        "folder_path = '/workspaces/CS-345_UNR_FA2024/Homework 9/' # my google drive folder path\n",
        "news_folder_path = folder_path + 'news_dataset/'\n",
        "\n",
        "df_fake = pandas.read_csv(news_folder_path + \"Fake.csv\")['text']\n",
        "df_real = pandas.read_csv(news_folder_path + \"True.csv\")['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Get Vocab Words for Fake\n",
        "word_dict = {}\n",
        "for text in df_fake:\n",
        "  text = clean_text(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "#Get Vocab Words for Real\n",
        "for text in df_real:\n",
        "  text = clean_text(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "\n",
        "#Remove words that occur less than min_thresh times and more than max_thresh times:\n",
        "# we still need to shorten the feature dimension manually, as google colab cannot deal with this large feature space.\n",
        "vocab = list(word_dict)\n",
        "\n",
        "min_thresh = 100\n",
        "max_thresh = 5000\n",
        "for word in vocab:\n",
        "  if word_dict[word] <= min_thresh or word_dict[word] > max_thresh:\n",
        "    word_dict.pop(word)\n",
        "\n",
        "vocab = list(word_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Function to process the texts and generate feature vectors\n",
        "def process_texts(texts, label):\n",
        "    X_data = []  # To store feature vectors\n",
        "    y_data = []  # To store labels\n",
        "\n",
        "    # Process each text\n",
        "    for text in texts:\n",
        "        # Clean the text\n",
        "        text = clean_text(text)\n",
        "\n",
        "        # Use defaultdict to avoid checking if word exists\n",
        "        article_dict = defaultdict(int)\n",
        "\n",
        "        # Count word occurrences\n",
        "        for word in text:\n",
        "            article_dict[word] += 1\n",
        "\n",
        "        # Turn the count dictionary into a list of values corresponding to the vocabulary\n",
        "        article_list = [article_dict[word] for word in vocab]\n",
        "\n",
        "        # Append the feature vector and the label\n",
        "        X_data.append(article_list)\n",
        "        y_data.append(label)\n",
        "\n",
        "    return X_data, y_data\n",
        "\n",
        "# Initialize data containers\n",
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "# Process both fake and real news texts and collect the data\n",
        "X_fake, y_fake = process_texts(df_fake, 1)  # Label 1 for fake news\n",
        "X_real, y_real = process_texts(df_real, 0)  # Label 0 for real news\n",
        "\n",
        "# Combine the data\n",
        "X_all.extend(X_fake)\n",
        "X_all.extend(X_real)\n",
        "y_all.extend(y_fake)\n",
        "y_all.extend(y_real)\n",
        "\n",
        "# At this point, X_all contains the feature vectors, and y_all contains the corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# model training\n",
        "nn = MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), batch_size=128, max_iter=1000, random_state=1)\n",
        "\n",
        "nn = nn.fit(X_train, y_train)\n",
        "print(nn.best_loss_)\n",
        "plt.plot(nn.loss_curve_, '-.', color='red', label='nn 2')\n",
        "plt.ylabel('training loss')\n",
        "plt.xlabel('# of iterations')\n",
        "\n",
        "# testing after training\n",
        "y_pred = nn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing during the training\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "nn = MLPClassifier(solver='adam', hidden_layer_sizes=(5, 2), batch_size=128,\n",
        "                   max_iter=1, warm_start=True, random_state=1)\n",
        "\n",
        "# Lists to store training and testing accuracy\n",
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "# Lists to store training and testing loss\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Train the model for multiple iterations\n",
        "for i in range(100):  # 100 iterations\n",
        "\n",
        "    # Incremental Training: partial_fit allows you to\n",
        "    # incrementally fit the model and track the performance after each iteration.\n",
        "\n",
        "    if i == 0:\n",
        "        nn.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
        "    else:\n",
        "        nn.partial_fit(X_train, y_train)\n",
        "\n",
        "    # Append the training loss (stored in nn.loss_curve_)\n",
        "    train_losses.append(nn.loss_)\n",
        "\n",
        "    # Evaluate on training set\n",
        "    train_pred = nn.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_pred)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_pred = nn.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    y_test_prob = nn.predict_proba(X_test)  # Get predicted probabilities\n",
        "    test_loss = log_loss(y_test, y_test_prob)  # Compute the log loss for the test set\n",
        "    test_losses.append(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the test accuracy over iterations\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test and Train Accuracy over Iterations')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and testing loss over iterations\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(test_losses, label=\"Testing Loss\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Testing Loss over Iterations')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
